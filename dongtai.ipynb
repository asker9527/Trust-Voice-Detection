{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Author: Moustafa Alzantot (malzantot@ucla.edu)\n",
    "    All rights reserved.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import data_utils\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import librosa\n",
    "# from torchstat import stat\n",
    "import torch\n",
    "from torch import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import models\n",
    "from models import SpectrogramModel, MFCCModel, CQCCModel, DenseNet, Bottleneck, CQCCBotNet, SpectBotNet, MFCCBotNet, LFCCBotNet, FeatureExtractor, seDenseTransNet, SEDenseNet\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "#from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pad(x, max_len=64000):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = (max_len / x_len)+1\n",
    "    x_repeat = np.repeat(x, num_repeats)\n",
    "    padded_x = x_repeat[:max_len]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_loader, model, device, epoch):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    model.eval()\n",
    "    for batch_x, batch_y, batch_meta in data_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_out, loss = model(batch_x, batch_y, epoch)\n",
    "        _, batch_pred = batch_out.max(dim=1)\n",
    "        num_correct += (batch_pred == batch_y).sum(dim=0).item()\n",
    "    return 100 * (num_correct / num_total)\n",
    "\n",
    "\n",
    "def produce_evaluation_file(dataset, model, device, save_path, epoch):\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    model.eval()\n",
    "    true_y = []\n",
    "    fname_list = []\n",
    "    key_list = []\n",
    "    sys_id_list = []\n",
    "    key_list = []\n",
    "    score_list = []\n",
    "    for batch_x, batch_y, batch_meta in data_loader:\n",
    "        #batch_x = np.expand_dims(batch_x, axis=1)\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_out, loss = model(batch_x, batch_y, epoch)\n",
    "        batch_score = (batch_out[:, 1] - batch_out[:, 0]\n",
    "                       ).data.cpu().numpy().ravel()\n",
    "\n",
    "        # add outputs\n",
    "        fname_list.extend(list(batch_meta[1]))\n",
    "        key_list.extend(\n",
    "            ['bonafide' if key == 1 else 'spoof' for key in list(batch_meta[4])])\n",
    "        sys_id_list.extend([dataset.sysid_dict_inv[s.item()]\n",
    "                            for s in list(batch_meta[3])])\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "    with open(save_path, 'w') as fh:\n",
    "        for f, s, k, cm in zip(fname_list, sys_id_list, key_list, score_list):\n",
    "            if not dataset.is_eval:\n",
    "                fh.write('{} {} {} {}\\n'.format(f, s, k, cm))\n",
    "            else:\n",
    "                fh.write('{} {} {} {}\\n'.format(f, s, k, cm))\n",
    "    print('Result saved to {}'.format(save_path))\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, lr, device, epoch):\n",
    "    running_loss = 0\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    ii = 0\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=lr, weight_decay=1e-5)\n",
    "    weight = torch.FloatTensor([1.0, 9.0]).to(device)\n",
    "    criterion = nn.NLLLoss(weight=weight)\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        #batch_x = np.expand_dims(batch_x, axis=1)\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        ii += 1\n",
    "\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        evidences, loss = model(batch_x, batch_y, epoch)\n",
    "        #batch_loss = criterion(batch_out, batch_y)\n",
    "        _, batch_pred = evidences .max(dim=1)\n",
    "        num_correct += (batch_pred == batch_y).sum(dim=0).item()\n",
    "        #running_loss += (loss.item() * batch_size)\n",
    "        if ii % 10 == 0:\n",
    "            sys.stdout.write('\\r \\t {:.2f}'.format(\n",
    "                (num_correct/num_total)*100))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    #running_loss /= num_total\n",
    "    train_accuracy = (num_correct/num_total)*100\n",
    "    return loss, train_accuracy\n",
    "\n",
    "\n",
    "def get_log_spectrum(x):\n",
    "    s = librosa.core.stft(x, n_fft=2048, win_length=2048, hop_length=512)\n",
    "    a = np.abs(s)**2\n",
    "    #melspect = librosa.feature.melspectrogram(S=a)\n",
    "    feat = librosa.power_to_db(a)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def compute_mfcc_feats(x):\n",
    "    mfcc = librosa.feature.mfcc(x, sr=16000, n_mfcc=24)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(delta)\n",
    "    feats = np.concatenate((mfcc, delta, delta2), axis=0)\n",
    "    return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_part'], dest='eval_part', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('UCLANESL ASVSpoof2019  model')\n",
    "parser.add_argument('--eval', action='store_true', default=False,\n",
    "                    help='eval mode')\n",
    "parser.add_argument('--model_path', type=str,\n",
    "                    default=None, help='Model checkpoint')\n",
    "parser.add_argument('--eval_output', type=str, default=None,\n",
    "                    help='Path to save the evaluation result')\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--num_epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--comment', type=str, default=None,\n",
    "                    help='Comment to describe the saved mdoel')\n",
    "parser.add_argument('--track', type=str, default='logical')\n",
    "parser.add_argument('--features', type=str, default='cqcc')\n",
    "parser.add_argument('--is_eval', action='store_true', default=False)\n",
    "parser.add_argument('--eval_part', type=int, default=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建路径、模型名称等\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "args = parser.parse_args([])    # 传入一个空列表，不然会报错\n",
    "track = args.track\n",
    "assert args.features in ['mfcc', 'spect', 'cqcc', 'mfcc_dense', 'spect_dense', 'cqcc_sedense', 'lfcc', 'lfcc_dense', 'lfcc_sedense', 'mfcc_botnet', 'spect_botnet', 'lfcc_botnet', 'cqcc_botnet', 'lfcc_deit', 'cqcc_deit', 'mfcc_deit', 'spect_deit', 'lfcc_sedensetrans', 'spect_sedense', 'fuse_sedense'], 'Not supported feature'\n",
    "model_tag = 'model_{}_{}_{}_{}_{}'.format(\n",
    "    track, args.features, args.num_epochs, args.batch_size, args.lr)\n",
    "if args.comment:\n",
    "    model_tag = model_tag + '_{}'.format(args.comment)\n",
    "model_save_path = os.path.join('models', model_tag)\n",
    "assert track in ['logical', 'physical'], 'Invalid track given'\n",
    "is_logical = (track == 'logical')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************CQCCModel**********************\n"
     ]
    }
   ],
   "source": [
    "# 默认模型是cqcc\n",
    "if args.features == 'mfcc':\n",
    "    feature_fn = compute_mfcc_feats\n",
    "    model_cls = MFCCModel\n",
    "    print(\"*************MFCCModel**********************\")\n",
    "elif args.features == 'spect':\n",
    "    feature_fn = get_log_spectrum\n",
    "    model_cls = SpectrogramModel\n",
    "    print(\"*************SpectrogramModel**********************\")\n",
    "\n",
    "elif args.features == 'cqcc':\n",
    "    feature_fn = None  # cqcc feature is extracted in Matlab script\n",
    "    model_cls = CQCCModel\n",
    "    print(\"*************CQCCModel**********************\")\n",
    "    \n",
    "elif args.features == 'lfcc':\n",
    "    feature_fn = None  # lfcc feature is extracted in Matlab script\n",
    "    model_cls = CQCCModel\n",
    "    print(\"*************CQCCModel**********************\")\n",
    "elif args.features == 'cqcc_sedense':\n",
    "    feature_fn = None  # cqcc feature is extracted in Matlab script\n",
    "    model_cls = SEDenseNet\n",
    "    print(\"*************CQCC_DenseNetModel**********************\")\n",
    "elif args.features == 'spect_dense':\n",
    "    feature_fn = get_log_spectrum\n",
    "    model_cls = DenseNet\n",
    "    print(\"*************spect_DneseNetModel**********************\")\n",
    "elif args.features == 'mfcc_dense':\n",
    "    feature_fn = compute_mfcc_feats\n",
    "    model_cls = DenseNet\n",
    "    print(\"*************mfcc_DenseNetModel**********************\")    \n",
    "elif args.features == 'lfcc_dense':\n",
    "    feature_fn = None\n",
    "    model_cls = DenseNet\n",
    "    print(\"*************lfcc_DenseNetModel**********************\")  \n",
    "elif args.features == 'mfcc_botnet':\n",
    "    feature_fn = compute_mfcc_feats\n",
    "    model_cls = MFCCBotNet\n",
    "    print(\"*************mfcc_botnet**********************\")\n",
    "elif args.features == 'spect_botnet':\n",
    "    feature_fn = get_log_spectrum\n",
    "    model_cls = SpectBotNet\n",
    "    print(\"*************spect_botnet**********************\")\n",
    "elif args.features == 'lfcc_botnet':\n",
    "    feature_fn = None\n",
    "    model_cls = LFCCBotNet\n",
    "    print(\"*************lfcc_botnet**********************\")\n",
    "elif args.features == 'cqcc_botnet':\n",
    "    feature_fn = None\n",
    "    model_cls = CQCCBotNet\n",
    "    print(\"*************cqcc_botnet**********************\")\n",
    "elif args.features == 'lfcc_deit':\n",
    "    feature_fn = None\n",
    "    model_cls = models.deit_base_patch16_224\n",
    "    print(\"*************lfcc_deit**********************\")\n",
    "elif args.features == 'cqcc_deit':\n",
    "    feature_fn = None\n",
    "    model_cls = models.cqcc_deit_base_patch16_224\n",
    "    print(\"*************cqcc_deit**********************\")\n",
    "elif args.features == 'mfcc_deit':\n",
    "    feature_fn = None\n",
    "    model_cls = models.mfcc_deit_base_patch16_224\n",
    "    print(\"*************mfcc_deit**********************\")\n",
    "elif args.features == 'spect_deit':\n",
    "    feature_fn = None\n",
    "    model_cls = models.spect_deit_base_patch16_224\n",
    "    print(\"*************spect_deit**********************\")\n",
    "elif args.features == 'lfcc_sedensetrans':\n",
    "    feature_fn = None\n",
    "    model_cls = seDenseTransNet\n",
    "    print(\"*************lfcc_seDenseTransNetModel**********************\")\n",
    "elif args.features == 'lfcc_sedense':\n",
    "    feature_fn = None\n",
    "    model_cls = SEDenseNet\n",
    "    print(\"*************lfcc_seDenseNetModel**********************\")\n",
    "elif args.features == 'spect_sedense':\n",
    "    feature_fn = get_log_spectrum\n",
    "    model_cls = SEDenseNet\n",
    "    print(\"*************spect_seDenseNetModel**********************\")\n",
    "elif args.features == 'fuse_sedense':\n",
    "    feature_fn = None\n",
    "    model_cls = SEDenseNet\n",
    "    print(\"*************fuse_seDenseNetModel**********************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.CQCCModel"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\dongtai.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transforms1 \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39;49mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: pad(x),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: librosa\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mnormalize(x),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: feature_fn(x),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: Tensor(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "\n",
    "transforms = transforms.Compose([\n",
    "    lambda x: pad(x),\n",
    "    lambda x: librosa.util.normalize(x),\n",
    "    lambda x: feature_fn(x),\n",
    "    lambda x: Tensor(x)\n",
    "])\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matlab cache for cqcc feature do not exist.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\dongtai.ipynb 单元格 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# 这个似乎是测试集\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dev_set \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39;49mASVDataset(is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_logical\u001b[39m=\u001b[39;49mis_logical,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                 transform\u001b[39m=\u001b[39;49mtransforms,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                 feature_name\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mfeatures, is_eval\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mis_eval, eval_part\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49meval_part)\n",
      "File \u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\data_utils.py:121\u001b[0m, in \u001b[0;36mASVDataset.__init__\u001b[1;34m(self, transform, is_train, sample_size, is_logical, feature_name, is_eval, eval_part)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_y \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_y[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m select_idx]\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_sysid \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_sysid[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m select_idx]\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_x)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataset.py:83\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[1;34m(self, attribute_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m function\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 这个似乎是测试集\n",
    "dev_set = data_utils.ASVDataset(is_train=False, is_logical=is_logical,\n",
    "                                transform=transforms,\n",
    "                                feature_name=args.features, is_eval=args.is_eval, eval_part=args.eval_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = DataLoader(dev_set, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paramerters in networks is 311394  \n",
      "Namespace(eval=False, model_path=None, eval_output=None, batch_size=32, num_epochs=100, lr=0.0001, comment=None, track='logical', features='cqcc', is_eval=False, eval_part=0)\n",
      "Matlab cache for cqcc feature do not exist.\n"
     ]
    }
   ],
   "source": [
    "model = model_cls().to(device)\n",
    "#model = nn.DataParallel(model, device_ids=[2,3])\n",
    "print(\"Total number of paramerters in networks is {}  \".format(sum(x.numel() for x in model.parameters())))\n",
    "'''if True:\n",
    "    pretrained_dict = torch.load(\"./models/model_logical_lfcc_dense_200_32_5e-05/epoch_199.pth\")\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict_update = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict.keys() and v.size() == model_dict[k].size():\n",
    "            pretrained_dict_update[k] = v\n",
    "    missed_params = [k for k, v in model_dict.items() if not k in pretrained_dict_update.keys()]\n",
    "    print('loaded params/tot params:{}/{}'.format(len(pretrained_dict_update), len(model_dict)))\n",
    "    print('miss matched params:{}'.format(missed_params))\n",
    "    model_dict.update(pretrained_dict_update)\n",
    "    model.load_state_dict(model_dict)\n",
    "    '''\n",
    "#model.load_state_dict(torch.load(\"./models/model_logical_lfcc_sedense_200_128_5e-05/epoch_199.pth\"))\n",
    "print(args)\n",
    "\n",
    "if args.model_path:\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "    print('Model loaded : {}'.format(args.model_path))\n",
    "\n",
    "if args.eval:\n",
    "    assert args.eval_output is not None, 'You must provide an output path'\n",
    "    assert args.model_path is not None, 'You must provide model checkpoint'\n",
    "    produce_evaluation_file(dev_set, model, device, args.eval_output, args.num_epochs)\n",
    "    sys.exit(0)\n",
    "\n",
    "train_set = data_utils.ASVDataset(is_train=True, is_logical=is_logical, transform=transforms,\n",
    "                                    feature_name=args.features)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=args.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\dongtai.ipynb 单元格 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39m'\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model_tag))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     running_loss, train_accuracy \u001b[39m=\u001b[39m train_epoch(train_loader, model, args\u001b[39m.\u001b[39;49mlr, device, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     valid_accuracy \u001b[39m=\u001b[39m evaluate_accuracy(dev_loader, model, device, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m'\u001b[39m, train_accuracy, epoch)\n",
      "\u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\dongtai.ipynb 单元格 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([\u001b[39m1.0\u001b[39m, \u001b[39m9.0\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss(weight\u001b[39m=\u001b[39mweight)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_x, batch_y \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m#batch_x = np.expand_dims(batch_x, axis=1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X13sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     num_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:512\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\sampler.py:229\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[List[\u001b[39mint\u001b[39m]]:\n\u001b[0;32m    228\u001b[0m     batch \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler:\n\u001b[0;32m    230\u001b[0m         batch\u001b[39m.\u001b[39mappend(idx)\n\u001b[0;32m    231\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\sampler.py:66\u001b[0m, in \u001b[0;36mSequentialSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mint\u001b[39m]:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)))\n",
      "File \u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\data_utils.py:124\u001b[0m, in \u001b[0;36mASVDataset.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataset.py:83\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[1;34m(self, attribute_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m function\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = args.num_epochs\n",
    "writer = SummaryWriter('logs/{}'.format(model_tag))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, train_accuracy = train_epoch(train_loader, model, args.lr, device, epoch)\n",
    "    valid_accuracy = evaluate_accuracy(dev_loader, model, device, epoch)\n",
    "    writer.add_scalar('train_accuracy', train_accuracy, epoch)\n",
    "    writer.add_scalar('valid_accuracy', valid_accuracy, epoch)\n",
    "    writer.add_scalar('loss', running_loss, epoch)\n",
    "    print('\\n{} - {} - {:.2f} - {:.2f}'.format(epoch,\n",
    "                                                running_loss, train_accuracy, valid_accuracy))\n",
    "    torch.save(model.state_dict(), os.path.join(\n",
    "        model_save_path, 'epoch_{}.pth'.format(epoch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\dongtai.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_x, batch_y \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/Trust_Voice_Detection/%E5%8E%9F%E5%85%88%E7%9A%84%E5%B7%A5%E4%BD%9C/TMC-main/tmc/Trust-Voice-Detection/dongtai.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:512\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\sampler.py:229\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[List[\u001b[39mint\u001b[39m]]:\n\u001b[0;32m    228\u001b[0m     batch \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler:\n\u001b[0;32m    230\u001b[0m         batch\u001b[39m.\u001b[39mappend(idx)\n\u001b[0;32m    231\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\sampler.py:66\u001b[0m, in \u001b[0;36mSequentialSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mint\u001b[39m]:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)))\n",
      "File \u001b[1;32me:\\github\\Trust_Voice_Detection\\原先的工作\\TMC-main\\tmc\\Trust-Voice-Detection\\data_utils.py:124\u001b[0m, in \u001b[0;36mASVDataset.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Deep Learning\\lib\\site-packages\\torch\\utils\\data\\dataset.py:83\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[1;34m(self, attribute_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m function\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_loader:\n",
    "    \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
